# -*- coding: utf-8 -*-
"""Task1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PSupnhVBWcOTVTF9tfnYoA_OJPCCUPP4
"""

## Importing libraries
import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
# import seaborn as sns
# import matplotlib.pyplot as plt
# import missingno as msno
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split,KFold,GridSearchCV,cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
import pickle

## Reading the CSV file
data = pd.read_csv(r"C:\Users\Dell\Desktop\Intern\train_AV3.csv")


##data.info()

## Checking the NULL values

##data.isnull().sum()

## Visualization of missing data before dealing them
#msno.bar(data)

## Dealing with missing and NULL values
Feature_with_null = [data.isnull().sum().index]
for i in Feature_with_null:
  data[i] = data[i].fillna(method='bfill')
# data.isnull().sum()

## Visualization of missing data after dealing them
# msno.bar(data)

# data.head()

## Value counts of all features
# columns = ['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education',
#        'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',
#        'Loan_Amount_Term', 'Credit_History', 'Property_Area', 'Loan_Status']
# for i in columns:
#   print(i)
#   print(data[i].value_counts())

# categorical_columns = ['Gender', 'Married', 'Dependents', 'Education','Self_Employed','Loan_Status']
# for i in categorical_columns:
#   sns.countplot(data=data,x=i)
#   plt.show()

## Changing categorical values into numerical values
data['Gender'].replace({'Male':1,'Female':0},inplace=True)
data['Married'].replace({'Yes':1,'No':0},inplace=True)
data['Dependents'].replace({'3+':4},inplace=True)
data['Education'].replace({'Graduate':1,'Not Graduate':0},inplace=True)
data['Self_Employed'].replace({'Yes':1,'No':0},inplace=True)
data['Property_Area'].replace({'Urban':2,'Semiurban':1,'Rural':0},inplace=True)
data['Loan_Status'].replace({'Y':1,'N':0},inplace=True)
data.head()

## Independent variables
x = data.drop(['Loan_ID','Loan_Status'],axis=1)
## Dependent variable
y = data['Loan_Status']

## Feature selection
fs=SelectKBest(chi2,k=7)
final=fs.fit(data.drop(['Loan_Status','Loan_ID'],axis=1),y)
# final

# final.scores_

scores=pd.DataFrame(final.scores_,columns=['scores'])
columns=pd.DataFrame(x.columns,columns=['columns'])
scores=pd.concat([scores,columns],axis=1)
# scores

result=scores.nlargest(6,columns='scores')
result1=list(result['columns'].values)
# result1

remaining= list(set(x.columns)-set(result1))

x=x.drop(remaining,axis=1)
# x.head()

# Scaling method
# scaler = StandardScaler()
# s = scaler.fit_transform(x)
# x = pd.DataFrame(s,columns = x.columns)
# x

## cross validation

cv=KFold(n_splits=5,shuffle=True)
# score1=cross_val_score(DecisionTreeClassifier(),x,y)
# score2=cross_val_score(LogisticRegression(),x,y)
# score3=cross_val_score(SVC(),x,y)
# score4=cross_val_score(KNeighborsClassifier(),x,y)


# print(score1)
# print(score2)
# print(score3)
# print(score4)

## hyperparameter tuning

params2={
    'penalty':['l1','l2'],
    'C':[1.0]
}

params3={
  'C':[0.001,0.01,1.0],
  'gamma':['auto','scale'],
  'class_weight':['balanced',None]
}

params4={
    'n_neighbors':[2,3,4,5,6,78,16],
    'p':[2,3]
}


# hp2=GridSearchCV(LogisticRegression(),param_grid=params2,cv=cv)
hp3=GridSearchCV(SVC(),param_grid=params3,cv=cv)
# hp4=GridSearchCV(KNeighborsClassifier(),param_grid=params4,cv=cv)

x_train,x_test,y_train,y_test = train_test_split(x,y,train_size = 0.8,random_state = 42)

# # Decision tree
# Model1 = DecisionTreeClassifier()
# Model1.fit(x_train,y_train)
# y_pred = Model1.predict(x_test)
# print(accuracy_score(y_pred,y_test))

# Logistic Regression
Model2 = LogisticRegression()
Model2.fit(x_train,y_train)
y_pred = Model2.predict(x_test)
print(accuracy_score(y_pred,y_test))

# # Support vector classifier
# Model3 = SVC()
# Model3.fit(x_train,y_train)
# y_pred = Model3.predict(x_test)
# print(accuracy_score(y_pred,y_test))

# Kth nearest neighbour
# Model4 = KNeighborsClassifier()
# Model4.fit(x_train,y_train)
# y_pred = Model4.predict(x_test)
# print(accuracy_score(y_pred,y_test))

# # Logistic Regression
# hp2.fit(x_train,y_train)
# m2=hp2.best_estimator_
# y_pred=m2.predict(x_test)
# print(accuracy_score(y_pred,y_test))

# Support vector classifier
# hp3.fit(x_train,y_train)
# m3=hp3.best_estimator_
# y_pred=m3.predict(x_test)
# print(accuracy_score(y_pred,y_test))
print(Model2.predict([[1508,4583,128,1,360,1]]))
#Kth nearest neighbour
# hp4.fit(x_train,y_train)
# m4=hp4.best_estimator_
# y_pred=m4.predict(x_test)
# print(accuracy_score(y_pred,y_test))

# pickle.dump(Model2, open("saved_model.pkl","wb"))